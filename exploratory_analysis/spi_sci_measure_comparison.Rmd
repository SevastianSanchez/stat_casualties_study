---
title: "Multiple POLS, Statistical Capacity Indicies"
author: "Sevastian Sanchez"
date: "`r Sys.Date()`"
output:
  pdf_document:
    toc: no
    toc_depth: '3'
    number_sections: yes
  html_document:
    toc: yes
    toc_depth: 3
    toc_float: yes
    number_sections: yes
    highlight: tango
    theme: default
    fig_caption: yes
    df_print: tibble
---

# Preliminary exploratory analysis and descriptive comparison of Statistical 
# Capacity Indices (SPI & SCI) on SDG Performance


# All Countries, Preliminary Analysis (SPI x SDGs)
SPI = Statistical Performance Index (0-100, continuous)
SDG = Sustainable Development Goals (0-100, continuous)
DI = EIU Democracy Index/Score (0-10, continuous)
log_gdppc = Log(GDP Per Capita)


```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE, warning = FALSE, message = FALSE,
  tidy = TRUE, tidy.opts = list(width.cutoff = 60) 
)
```


### LOAD FINAL MERGED CSV 
```{r}
# set working directory
setwd("~/Documents/GitHub/QMSS_Thesis_Sanchez")

# load libraries
source("packages.R")

#load final merged df 
merged <- read_csv("data/Main CSV Outputs/comp1_data.csv") # updated 

# refer to 'wrangled/adjust_output.Rmd' to make necessary adjustments 
```

# COMPONENT 1: COMPARING SPI & SCI X VARIABLES

## Preliminary Analysis: Correlation & Naive OLS Models [FINALIZED]

### Correlation Analysis: SPI, SCI, DI & SDG Composite Scores
*H0: Null, there is no relationship*  
*H1: there is a relationship between overall SPI and SDG composite scores*

```{r}
# Correlation coefficients & R-squared values for SDG and SPI/SCI/DI

# SDG ~ SPI
correlation_sdg_spi <- cor(merged$sdg_overall, merged$spi_comp, use = "complete.obs")
# R-squared value
R2_sdg_spi <- correlation_sdg_spi^2

# SDG ~ SCI
correlation_sdg_sci <- cor(merged$sdg_overall, merged$sci_overall, use = "complete.obs") 
# R-squared value
R2_sdg_sci <- correlation_sdg_sci^2

# SDG ~ DI
correlation_sdg_di <- cor(merged$sdg_overall, merged$di_score, use = "complete.obs") 
# R-squared value
R2_sdg_di <- correlation_sdg_di^2

# SPI ~ DI 
correlation_spi_di <- cor(merged$spi_comp, merged$di_score, use = "complete.obs")
# R-squared value
R2_spi_di <- correlation_spi_di^2

# SCI ~ DI 
correlation_sci_di <- cor(merged$sci_overall, merged$di_score, use = "complete.obs")
# R-squared value
R2_sci_di <- correlation_sci_di^2

# pasting correlation results
paste("Correlation coefficient:" , correlation_sdg_spi, "(SDG ~ SPI),", correlation_sdg_sci, "(SDG ~ SCI),", correlation_sdg_di, "(SDG ~ DI),", correlation_spi_di, "(SPI ~ DI),", correlation_sci_di, "(SCI ~ DI)")

# pasting R-sq results
paste("R-squared value:", R2_sdg_spi, "(SDG ~ SPI),", R2_sdg_sci, "(SDG ~ SCI),", R2_sdg_di, "(SDG ~ DI),", R2_spi_di, "(SPI ~ DI),", R2_sci_di, "(SCI ~ DI)")
```
SDG ~ SPI: Correlation coefficient: 0.784880, R-squared value: 0.616037
SDG ~ SCI: Correlation coefficient: 0.646503, R-squared value: 0.417966
SDG ~ DI: Correlation coefficient: 0.672644, R-squared value: 0.452450
SPI ~ DI: Correlation coefficient: 0.676171, R-squared value: 0.4572067
SCI ~ DI: Correlation coefficient: 0.477767, R-squared value: 0.2282613

The results demonstrates that SDG performance (SDG) composite scores are most strongly associated with the Statistical Performance Index (SPI), which shows a high correlation coefficient (0.78) and explains about 62% of the variance in SDG scores. Both the Statistical Capacity Index (SCI) and the Democracy Index (DI) also exhibit moderate positive correlations with SDG scores (0.65 and 0.67, respectively), accounting for 42% and 45% of the variance. This could indicate that countries with higher statistical performance and stronger democratic institutions tend to achieve better SDG outcomes, with SPI emerging as the most influential predictor among the indices examined.

Both SPI and SCI are also positively correlated with the Democracy Index, with SPI (0.68) showing a stronger association than SCI (0.48). These findings highlight the interconnectedness of statistical capacity and governance quality, suggesting that improvements in national statistical systems and democratic governance are mutually reinforcing factors in advancing sustainable development.

**The correlation analysis provides a useful overview of the relationships between these indices, but further analysis is needed to understand the causal mechanisms and the impact of these indices on SDG outcomes.**

### NAIVE OLS Models (Component 1): Comparing SPI & SCI Variables on SDG Performance
Finding estimated impact of variables on SDG status prior to adding controls or robust SEs
```{r}
# 1. OLS for SPI and SDG - Overall 
ols_spi_naive <- lm(sdg_overall ~ spi_comp, data = merged)
summary(ols_spi_naive)

# 2. OLS for SCI and SDG - Overall 
ols_sci_naive <- lm(sdg_overall ~ sci_overall, data = merged)
summary(ols_sci_naive)

# 3. Multiple Regression with both SPI and SCI
ols_multiple_naive <- lm(sdg_overall ~ spi_comp + sci_overall, data = merged)
summary(ols_multiple_naive)

# stargazer summary table of models: ols_spi_naive, ols_sci_naive, ols_multiple_naive.
stargazer(
  ols_spi_naive, 
  ols_sci_naive, 
  ols_multiple_naive, 
  title = "Naive OLS Models: SPI & SCI x SDG",
  dep.var.caption = "Dependent Variable:",
  dep.var.labels = "SDG Composite Score",
  covariate.labels = c("SPI Composite Score", "SCI Composite Score", "Intercept"),
  column.labels = c("SPI Only", "SCI Only", "SPI + SCI"),  # <-- Model labels at the top
  omit.stat = c("f", "ser"), 
  digits = 4,
  type = "text" 
  #out = "component_1/figures/naive_mods_sdgs_spi_sci_tab.html"  # Save as HTML file
)

```
ols_spi_naive: 0.47806 (p-value < 0.01)  
ols_sci_naive: 0.39081 (p-value < 0.01)  
ols_multiple_naive: spi: 0.28779 (p-value < 0.01); 
                    sci: 0.15311 (p-value < 0.01)

The impact of SCI on SDG and SPI on SDG are statistically significant, in all models. SPI appears to have a greater impact on SDGs compared to that of SCI, regardless of the model. All of this is without controls or accounting multiple time periods of the same subject (i.e., countries).

### Checking for Heteroskedasticity: residual plots [no need to report]
```{r}
#residual plots 
plot(ols_spi_naive, which = 1)  # SDG ~ SPI model
plot(ols_sci_naive, which = 1)  # SDG ~ SCI model
plot(ols_multiple_naive, which = 1)  # SDG ~ SPI + SCI model 

# U-shaped residuals detected, suggests non-linearity of x-variable terms. Additional tests reconfirm non-linearity (See Breusch-Pagan Test below). 
```

--------------------------------------------------------------------------------

## TEST 1: Pooled OLS & Clustered Robust Standard Errors -- COMPARING MEASURES [DONE]

*Methodology: Pooled OLS Models & Clustered Robust (Huber-White) Standard Errors*
All variables of statistical capacity (SPI & SCI) will be compared on a base pooled OLS regression model structure. Pooled OLS recognizes the panel-like structure allowing to index by specific country and year (country-year). Regular OLS, assumes independence of observations which is not suitable given the repeated waves of country-year over the course of multiple consecutive years. Furthermore, it is customary to apply clustered-group robust standard errors to account for heteroskedasticity and within-unit correlation of countries over many time points. 

*H0: Null, there is no relationship between SPI and SDG composite scores*
*H1: There is a statistically significant relationship between SPI and SDG composite scores*

```{r}
# 1. OLS for SPI and SDG - Overall 
ols_spi <- plm(formula = sdg_overall ~ spi_comp + di_score + log_gdppc + factor(year), 
               model = "pooling", 
               index = c("country_code", "year"),
               data = merged)
summary(ols_spi, vcov = vcovHC(ols_spi, cluster = "group", type = "HC1"))

# 2. OLS for SCI and SDG - Overall 
ols_sci <- plm(formula = sdg_overall ~ sci_overall + di_score + log_gdppc + factor(year), 
               model = "pooling", 
               index = c("country_code", "year"),
               data = merged)
summary(ols_sci, vcov = vcovHC(ols_sci, cluster = "group", type = "HC1"))

# 3. Multiple Regression with both SPI and SCI
ols_multiple <- plm(formula = sdg_overall ~ spi_comp + sci_overall + di_score + log_gdppc + factor(year),
               model = "pooling", 
               index = c("country_code", "year"),
               data = merged)
summary(ols_multiple, vcov = vcovHC(ols_multiple, cluster = "group", type = "HC1"))
```

### Results in Stargazer Table & CSVs
```{r include=FALSE}
# For model statistics extraction 
summary_spi <- summary(ols_spi)
summary_sci <- summary(ols_sci)
summary_multiple <- summary(ols_multiple)

# Extracting robust SEs and coefficients (using coeftest)
rob_stats_spi <- coeftest(ols_spi, vcov = vcovHC(ols_spi, cluster = "group", type = "HC1"))
rob_stats_sci <- coeftest(ols_sci, vcov = vcovHC(ols_sci, cluster = "group", type = "HC1"))
rob_stats_multiple <- coeftest(ols_multiple, vcov = vcovHC(ols_multiple, cluster = "group", type = "HC1"))

# CREATING CSV OF MODEL'S STATISTICS TO SAVE 
# SPI Statistics DF
spi_df <- data.frame(
  model = rep("M1: ols_spi", nrow(rob_stats_spi)),
  term = rownames(rob_stats_spi),
  estimate = rob_stats_spi[, 1],
  std.error = rob_stats_spi[, 2],
  t.statistic = rob_stats_spi[, 3],
  p.value = rob_stats_spi[, 4],
  residual.SE = rep(sqrt(sum(residuals(ols_spi)^2) / ols_spi$df.residual), nrow(rob_stats_spi)),
  r.squared = rep(as.numeric(summary_spi$r.squared["rsq"]), nrow(rob_stats_spi)),
  adj.r.squared = rep(as.numeric(summary_spi$r.squared["adjrsq"]), nrow(rob_stats_spi)),
  row.names = NULL
)

#SCI Statistics DF
sci_df <- data.frame(
  model = rep("M2: ols_sci", nrow(rob_stats_sci)),
  term = rownames(rob_stats_sci),
  estimate = rob_stats_sci[, 1],
  std.error = rob_stats_sci[, 2],
  t.statistic = rob_stats_sci[, 3],
  p.value = rob_stats_sci[, 4],
  residual.SE = rep(sqrt(sum(residuals(ols_sci)^2) / ols_sci$df.residual), nrow(rob_stats_sci)),
  r.squared = rep(as.numeric(summary_sci$r.squared["rsq"]), nrow(rob_stats_sci)),
  adj.r.squared = rep(as.numeric(summary_sci$r.squared["adjrsq"]), nrow(rob_stats_sci)),
  row.names = NULL
)

#Combined Mod Statistics DF
multiple_df <- data.frame(
  model = "M3: ols_multiple",
  term = rownames(rob_stats_multiple),
  estimate = rob_stats_multiple[, 1],
  std.error = rob_stats_multiple[, 2],
  t.statistic = rob_stats_multiple[, 3],
  p.value = rob_stats_multiple[, 4],
  residual.SE = rep(sqrt(sum(residuals(ols_multiple)^2) / ols_multiple$df.residual),
                    nrow(rob_stats_multiple)),
  r.squared = rep(as.numeric(summary_sci$r.squared["rsq"]), nrow(rob_stats_multiple)),
  adj.r.squared = rep(as.numeric(summary_sci$r.squared["adjrsq"]), nrow(rob_stats_multiple)),
  row.names = NULL
)

# Bind all together into one tidy dataframe
robust_mods_df <- bind_rows(spi_df, sci_df, multiple_df)

# Attributes under column names 
attr(robust_mods_df$std.error, "label") <- "Robust Std. Errors Adjusted"
attr(robust_mods_df$t.statistic, "label") <- "Robust Std. Errors Adjusted"
attr(robust_mods_df$p.value, "label") <- "Robust Std. Errors Adjusted"

#save to results_csv folder
#write.csv(robust_mods_df, file = "component_1/results_csv/pols_mods_results.csv")

# View the result
print(robust_mods_df)

# Stargazer Table
stargazer(
  ols_spi, 
  ols_sci, 
  ols_multiple,
  se = list(rob_stats_spi[,2], rob_stats_sci[,2], rob_stats_multiple[,2]),
  title = "Pooled OLS Models with Robust Standard Errors",
  align = TRUE,
  dep.var.labels = "SDG Overall Performance",
  covariate.labels = c("SPI Composite", "SCI Overall", "Democracy Index", 
                      "Log GDP per Capita", "Year 2008", "Year 2010", "Year 2011", 
                       "Year 2012", "Year 2013", "Year 2014", "Year 2015", "Year 2016", 
                       "Year 2017", "Year 2018", "Year 2019", "Year 2020", "Year 2021", 
                       "Year 2022", "Year 2023"),
  column.labels = c("M1: ols_spi", "M2: ols_sci", "M3: ols_multiple"),
  keep.stat = c("n", "rsq", "adj.rsq", "f"),
  type = "text"
  #out = "component_1/figures/pols_mods_results_tab.html"
)
``` 

**Results:**
The results of the OLS models with robust standard errors indicate that both SPI and SCI have a statistically significant positive relationship with SDG composite scores. The p-values for both SPI and SCI are less than 0.001, indicating strong evidence against the null hypothesis of no relationship for either model. Holding all else constant (log GDP per capita, democracy score and year), SPI and SCI exhibit positive moderate and statistically significant relationships with SDG status.

ols_spi: 0.28735 (p-value < 0.001)*** 
ols_sci: 0.237633 (p-value < 0.001)*** 
ols_multiple: 
  spi: 0.124233 (p-value < 0.05)*
  sci: 0.138766 (p-value < 0.01)**

When compared in separate models, SPI has a greater impact on SDG status (0.28735) than SCI (0.237633). This suggests that a one-unit increase in SPI is associated with a larger improvement in SDG outcomes compared to a one-unit increase in SCI, holding all controls constant. 

Interestingly, the opposite holds true in a multiple regression model containing both SPI and SCI. SPI's impact on SDG status (0.124233) (net of SPI) is less than that of SCI's (0.138766) (net of SCI), holding all controls constant. When together, the coefficients represent the unique impact of each predictor variable (measures of statistical capacity) on SDG status, net of all other variables.

Model 1 (ols_spi) does not control for SCI and model 2 (ols_sci) does not control for spi -- this is okay. SPI is the predecessor of the SCI, sharing/data overlap, and so it is expected to have significant statistical correlation (multicollinearity). This is possibly what explains the reduction of both regression coefficients in model 3: 0.28735 to 0.124233 for SPI (__% decrease); and from 0.237633 to 0.138766 for SCI (__% decrease). This indicates that both variables capture much of the same underlying relationship with SDG performance.

However, the fact that both SPI and SCI remain significant when included together (model 3), although SCI less so than SPI, with a high adjusted R-sq (0.741525), which suggests they capture different dimensions of statistical capacity that independently contribute to SDG status.

### Checking for Multicolinearity: VIF of SPI & SCI [DONE]
```{r}
# Checking correlation between SPI and SCI
cor(merged$spi_comp, merged$sci_overall, use = "complete.obs")

# Checking VIF (Variance Inflation Factor) in Model 3
vif(ols_multiple)

# Datatable 
vif_vals <- vif(ols_multiple)   # returns a named vector
tidy_vif <- enframe(vif_vals, name = "term", value = "vif")

print(tidy_vif)

#write.csv(tidy_vif, file = "component_1/results_csv/vif_multiple_mod_3_results.csv")
```

**Colinearity:** The correlation between SCI and SPI is about 0.8277. When placed within the same model, SCI inflated the standard error of SPI from 0.013079 to 0.027054. SCI had a similar reaction from the SPI with its standard error increasing from 0.0095864 to 0.024096.

**VIF:** Such multicollinearity is moderately reflected in the VIF test which accounts for **all possible x variables** of a full model instead of only two measures of statistical capacity (SCI & SPI). A fully specified model, one that includes all hypothetical predictors of SDG performance, would ideally have sufficiently low multicollinearity to ensure reliable coefficient estimates: VIF scores below 5 are generally considered acceptable. 

_VIF Results:_
  term          vif    Df    GVIF^(1/(2*Df))
spi_comp     4.316894  1        2.077714
sci_overall  3.754613  1        1.937682
di_score     1.598314  1        1.264244
log_gdppc    1.485085  1        1.218641
factor(year) 1.302733  4        1.033611 

Overall there reveals no severe multicollinearity (all GVIF < 5). There is moderate correlation between statistical capacity measures (spi_comp and sci_overall) with SPI moderately inflated by a factor of 4.32 and SCI inflated by a factor of  3.75. Nevertheless, it is acceptable to include both in the same model as doing so will not severely impact estimates with both factors less than 5.0. Even so, there are significant limitations in either model that warrant strong consideration, including sample size, and longitudinal suitability. All other variables show minimal multicollinearity concerns.

### Checking for Heteroskedasticity: Breusch-Pagan Test [DONE]
This validates the need for integrating robust standard errors in our models
```{r}
# Breusch-Pagan Test for Heteroskedasticity
bp_spi <- bptest(ols_spi) # in objects for presentability 
bp_sci <- bptest(ols_sci)
bp_multiple <- bptest(ols_multiple)

# combine for data frame 
bp_tests <- list(
  ols_spi = bp_spi,
  ols_sci = bp_sci,
  ols_multiple = bp_multiple
)

# Tidy all tests and add a "model" column
bptests_results <- bp_tests %>% 
  map_df(~ tidy(.x), .id = "model")

print(bptests_results)

#write.csv(bptests_results, file = "component_1/results_csv/bp_heterosked_results.csv")
```
Model:         BP statistic      p-value
ols_spi           207.         = 7.05e-39       
ols_sci           86.6         = 4.22e-12
ols_multiple      31.1         = 1.33e- 4 

The Breusch-Pagan Test was applied to test to see whether residuals are constant across observations, which signals unaccounted non-linear relationships, especially with macro factors such as GDP Per Capita and Population in the models. This is important because Ordinary Least Squares models assume constant error variance. In such a complex world of diverse cultural and ever-changing political structures across almost 200 countries, cross-national data, especially in development, is rarely ever linear. Accordingly, this test evaluates the extent of such non-linearity among specified predictors. 

As such, results indicate strong evidence of heteroskedasticity in all three models. The small p-values in all models indicates that the variance of residuals are not constant across observations in all three models. This reinforces the motivation behind applying robust standard errors, which have been integrated to all OLS models. Without Robust SEs, there is a risk of inflated t-statistics, leading to false significance and misinterpretation of results.

Despite the improvement from 206.5 (SPI) and 86.6 (SCI) to 31.1 (Both), there still remains statically significant heteroskedasticity in the combined model. Both statistical capacity measures create a better-specified model (ols_multiple), though not enough to eliminate heteroskedasticity entirely.

### Missing Data Structure & Interpretations [DONE]
**Systematic, non-random missing data pattern:** SPI has near complete country data coverage (165 out of 168 countries with an SDG score), but with a stubborn temporal limitation (2016-2023). On the other hand, SCI has longer temporal coverage (2004-2020) but lacks reporting on high-income countries focusing primarily on the developing world (123 out of 168 countries with an SDG score).

### AIC/BIC Checking Fit [DONE]
*H0: Null, SCI model or Combined model > SPI model*
*H1: SPI model > SCI model & combined model*
Holding all controls constant between the three models, the only difference is the statistical capacity measure used (SPI, SCI, or both). Thus, AIC and BIC tests can be used to compare model fit across these three models. Lower AIC and BIC values indicate a better fit.
```{r}
# switching to lm for AIC & BIC tests [POLS]
ols_spi_lm <- lm(sdg_overall ~ spi_comp + di_score + log_gdppc + factor(year), data = merged)
ols_sci_lm <- lm(sdg_overall ~ sci_overall + di_score + log_gdppc + factor(year), data = merged)
ols_multiple_lm <- lm(sdg_overall ~ spi_comp + sci_overall + di_score + log_gdppc + factor(year), data = merged)

# Switching to lm for AIC & BIC tests [FE]
fe_spi_lm <- lm(sdg_overall ~ spi_comp + di_score + log_gdppc + factor(year) + factor(country_code), data = merged)
fe_sci_lm <- lm(sdg_overall ~ sci_overall + di_score + log_gdppc + factor(year) + factor(country_code), data = merged)
fe_multiple_lm <- lm(sdg_overall ~ spi_comp + sci_overall + di_score + log_gdppc + factor(year) + factor(country_code), data = merged)

# AIC & BIC tests - POLS models
AIC(ols_spi_lm, ols_sci_lm, ols_multiple_lm)
BIC(ols_spi_lm, ols_sci_lm, ols_multiple_lm)

# AIC & BIC tests - FE models
AIC(fe_spi_lm, fe_sci_lm, fe_multiple_lm)
BIC(fe_spi_lm, fe_sci_lm, fe_multiple_lm)

# Results into lists
aic_vals <- c(
  AIC(ols_spi_lm),
  AIC(ols_sci_lm),
  AIC(ols_multiple_lm),
  AIC(fe_spi_lm),
  AIC(fe_sci_lm),
  AIC(fe_multiple_lm)
)

bic_vals <- c(
  BIC(ols_spi_lm),
  BIC(ols_sci_lm),
  BIC(ols_multiple_lm),
  BIC(fe_spi_lm),
  BIC(fe_sci_lm),
  BIC(fe_multiple_lm)
)

# Model names
model_names <- c("ols_spi", "ols_sci", "ols_multiple", #POLS
                 "fe_spi", "fe_sci", "fe_multiple") #FE

# Combine into dataframe
aic_bic_comp1_results <- data.frame(
  model = model_names,
  AIC = aic_vals,
  BIC = bic_vals
)
print(aic_bic_comp1_results)

# saving to results_csv
#write.csv(aic_bic_comp1_results, file = "component_1/results_csv/aic_bic_results.csv")
```
**AIC/BIC Results**
     model          AIC       BIC
m1: ols_spi       7382.060  7443.496
m2: ols_sci       9003.178  9093.672
m3: ols_multiple  3292.001  3335.405
m4: fe_spi        2588.303  3443.282
m5: fe_sci        4337.547  5045.529
m6: fe_multiple   1073.202  1607.066
note: all models have different number of observations, ols_multiple containing the least.

**Adjusted R-squares** 
m1: ols_spi:      0.7757 [best fit]
m2: ols_sci:      0.74152
m3: ols_multiple: 0.75683
m4: fe_spi        
m5: fe_sci        
m6: fe_multiple

### Selecting Best Model [DONE]
**Best fit: ols_spi (Adj Rsq: 0.7725) (AIC/BIC: 7382.060, 7443.496) (n=1082)**
To determine which predictor (i.e., SPI or SCI) is stronger –also considering if they’re stronger together (ols_multiple)– this investigation considers (a) descriptive statistics (i.e., number of observations); (b) adjusted R2 estimates; and (c) AIC/BIC model scores. These measures are compared across models that sustain the same controls, however, statistical checks are weighted against frequency of observations, given anticipated differences in the number of years and mis-alignment of particular start to end year periods. 

### Visual Analysis of Fit: SCI & SPI x SDG
```{r echo=FALSE}
#define regression line colors
spi_line <- "steelblue4"
sci_line <- "darkgoldenrod"

# Creating scatterplot with both SPI and SCI on the same plot
compare_fit <- ggplot(merged, aes(x = spi_comp, y = sdg_overall))+
  geom_smooth(aes(x = spi_comp, y = sdg_overall), 
              color = spi_line, 
              method = "lm", 
              linewidth = 0.75, 
              se = FALSE)+ # Regression line SPI
  geom_smooth(aes(x = sci_overall, y = sdg_overall), 
              color = sci_line, 
              method = "lm", 
              linewidth = 0.75, 
              se = FALSE)+ # Regression line for SCI
  geom_point(aes(color = "spi_comp"), alpha=0.50, size = 0.5)+ # Scatter plot for SPI
  geom_point(aes(x = sci_overall, y = sdg_overall, color = "sci_overall"), 
             alpha=0.5, size = 0.5)+ # Add SCI points w/different color
  scale_color_manual(values = c("spi_comp" = "steelblue2", 
                                "sci_overall" = "darkgoldenrod2")) +
  labs(title = "Comparing SPI & SCI Measures Against SDG Index",
       x = "SPI & SCI Scores (0-100)",
       y = "SDG Composite (0-100)",
       color = "Measure:"
       ) + 
  theme_minimal() +
  theme(axis.title = element_text(size = 12)) + # size for both axis titles
  theme(legend.position = "bottom",
        legend.box = "horizontal",
        legend.title = element_text(size = 10), # size for legend title 
        legend.text = element_text(size = 10), # size for both legend lables
        plot.title = element_text(hjust= 0.4, size = 12, face = "bold")) # title size and position

# Print the plot
compare_fit

# Save to specific folder
#ggsave("component_1/figures/spi_n_sci_scatterplot.png", compare_fit, width = 8, height = 6)
```

*Note* The SPI regression line is expected to appear higher in terms of SDG Score compared the SCI model because SPI countries include higher-income countries. As previously mentioned, the SCI soley focuses on lower to upper-middle income countries (146 countries over 17 years). 

